---
title: Quickstart on MacOS
description: Get started with a Mac friendly deployment guide
---

import { Tabs, TabItem } from '@astrojs/starlight/components';
import compose_ollama_cpu from '../../../assets/compose_ollama_cpu.yaml?raw';
import env_ollama_cpu from '../../../assets/env_ollama_cpu.env?raw';
import { Code } from '@astrojs/starlight/components';

:::note
The easiest way to deploy OpenRAG on MacOS is to use Docker. Since Docker for MacOS does not support the MPS backend, perfomance may be limited. See [here](#optimizations) for more details.
:::

## Docker

### Prerequisites
- [Docker](https://www.docker.com/get-started) and **Docker Compose**
- Your hardware should meet these specifications:
  - A minimum of 24 GB of unified memory (32 GB recommended). 16 GB may work with varying degrees of success.
  - An Apple Silicon based Mac

### Installation

We provide precompiled Docker images for [OpenRAG](https://hub.docker.com/r/linagoraai/openrag/tags) and its dashboard companion, [Indexer-UI](https://hub.docker.com/r/linagoraai/indexer-ui/tags).

You will need the following `docker-compose.yaml` and `.env` files to get started:

<Tabs>
  <TabItem value="docker-compose.yaml" label="docker-compose.yaml">
  <Code code={compose_ollama_cpu} lang="yaml" />
  </TabItem>
    <TabItem value=".env" label=".env">
    <Code code={env_ollama_cpu} />
    </TabItem>
</Tabs>

### Configuration

By default, the only necessary configuration change is to set the model settings in the `.env` file. Make sure all three models are set (they can be the same one if it supports vision, language, and embedding) If using ollama, ensure you pull the desired models locally using the ollama CLI (keep in mind that ollama needs to be running to pull models):

```bash title="Pulling models with ollama"
ollama pull qwen3:0.6b
```
```env title=".env"
BASE_URL=http://ollama:11434
API_KEY=EMPTY
MODEL=qwen3:0.6b
```

:::caution[Important]
Ollama does not support reranker models as of October 2025. Rerankers must be run on a separate server or disabled (as configured by default in the provided `docker-compose.yaml`).
:::

### Optimizations

As stated earlier, Docker for MacOS does not support GPU acceleration. Therefore, to maximize performance, we recommend using a non-dockerized installation of ollama or LlamaCpp, or running models from an external server. For simplicity, we still provide a dockerized setup here.